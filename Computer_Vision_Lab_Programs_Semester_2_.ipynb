{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_upzrSxBNnt",
        "outputId": "e21ce7ee-1a3a-425a-d0f2-38dbcbb0c35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not load image.\n"
          ]
        }
      ],
      "source": [
        "# Write a python program to read and convert the BGR image to RGB using open CV\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the BGR image\n",
        "img_bgr = cv2.imread('image.png')\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img_bgr is None:\n",
        "    print(\"Error: Could not load image.\")\n",
        "else:\n",
        "    # Convert the BGR image to RGB\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the original BGR image (optional)\n",
        "    # Note: Matplotlib expects RGB, so displaying BGR directly will show incorrect colors\n",
        "    # plt.imshow(img_bgr)\n",
        "    # plt.title(\"Original BGR Image (Incorrect Colors in Matplotlib)\")\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "\n",
        "    # Display the converted RGB image\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(\"Converted RGB Image\")\n",
        "    plt.axis('off') # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "    # You can also save the converted image\n",
        "    # cv2.imwrite('image_rgb.png', img_rgb)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Program to read and convert the BGR image to grayscale using OpenCV\n",
        "\n",
        "# Load the BGR image\n",
        "img_bgr = cv2.imread('image.png')\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img_bgr is None:\n",
        "    print(\"Error: Could not load image.\")\n",
        "else:\n",
        "    # Convert the BGR image to grayscale\n",
        "    img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Display the original BGR image (optional)\n",
        "    # plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)) # Convert BGR to RGB for correct display\n",
        "    # plt.title(\"Original BGR Image\")\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "\n",
        "    # Display the converted grayscale image\n",
        "    plt.imshow(img_gray, cmap='gray') # Use 'gray' colormap for grayscale\n",
        "    plt.title(\"Converted Grayscale Image\")\n",
        "    plt.axis('off') # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "    # You can also save the converted image\n",
        "    # cv2.imwrite('image_gray.png', img_gray)"
      ],
      "metadata": {
        "id": "3YsBuV_9nI5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the image\n",
        "img = cv2.imread('image.png')\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img is None:\n",
        "    print(\"Error: Could not load image.\")\n",
        "else:\n",
        "    # Define the desired width and height\n",
        "    new_width = 300\n",
        "    new_height = 200\n",
        "\n",
        "    # Resize the image\n",
        "    resized_img = cv2.resize(img, (new_width, new_height))\n",
        "\n",
        "    # Display the original image (optional)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Display the resized image\n",
        "    plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Resized Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "6HQ20mbAnJGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: program to read 2 images and fused the 2 images using open CV\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Program to read 2 images and fuse them using OpenCV\n",
        "\n",
        "# Load the two images\n",
        "img1 = cv2.imread('image1.png')  # Replace 'image1.png' with the path to your first image\n",
        "img2 = cv2.imread('image2.png')  # Replace 'image2.png' with the path to your second image\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img1 is None or img2 is None:\n",
        "    print(\"Error: Could not load one or both images.\")\n",
        "else:\n",
        "    # Ensure images have the same dimensions for fusion\n",
        "    if img1.shape != img2.shape:\n",
        "        print(\"Error: Images must have the same dimensions for fusion.\")\n",
        "    else:\n",
        "        # Define the fusion weight (alpha), a value between 0 and 1\n",
        "        # A value of 0.5 means equal weighting of both images\n",
        "        alpha = 0.5\n",
        "\n",
        "        # Perform image fusion (weighted addition)\n",
        "        # The formula is: fused_image = alpha * img1 + (1 - alpha) * img2 + beta\n",
        "        # beta is an optional scalar added to each sum (usually 0)\n",
        "        fused_img = cv2.addWeighted(img1, alpha, img2, (1 - alpha), 0)\n",
        "\n",
        "        # Display the original images (optional)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Image 1\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Image 2\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display the fused image\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(cv2.cvtColor(fused_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"Fused Image (alpha={alpha})\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # You can also save the fused image\n",
        "        # cv2.imwrite('fused_image.png', fused_img)"
      ],
      "metadata": {
        "id": "4xKgr8pZnJJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a mtalb program to perform edge detection using sobel operator\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Convert to double for calculations\n",
        "img_double = im2double(img_gray);\n",
        "\n",
        "% Define the Sobel kernels\n",
        "sobel_x = [-1 0 1; -2 0 2; -1 0 1];\n",
        "sobel_y = [-1 -2 -1; 0 0 0; 1 2 1];\n",
        "\n",
        "% Apply the Sobel filters using convolution\n",
        "grad_x = conv2(img_double, sobel_x, 'same');\n",
        "grad_y = conv2(img_double, sobel_y, 'same');\n",
        "\n",
        "% Calculate the gradient magnitude\n",
        "edge_magnitude = sqrt(grad_x.^2 + grad_y.^2);\n",
        "\n",
        "% Display the original and edge-detected images\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imshow(img);\n",
        "title('Original Image');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "imshow(edge_magnitude, []); % Use [] to scale the display range\n",
        "title('Sobel Edge Detection');"
      ],
      "metadata": {
        "id": "XwiwS9yboZUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: matlab program to perform edge detection using perwitt filter\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Convert to double for calculations\n",
        "img_double = im2double(img_gray);\n",
        "\n",
        "% Define the Prewitt kernels\n",
        "prewitt_x = [-1 0 1; -1 0 1; -1 0 1];\n",
        "prewitt_y = [-1 -1 -1; 0 0 0; 1 1 1];\n",
        "\n",
        "% Apply the Prewitt filters using convolution\n",
        "grad_x = conv2(img_double, prewitt_x, 'same');\n",
        "grad_y = conv2(img_double, prewitt_y, 'same');\n",
        "\n",
        "% Calculate the gradient magnitude\n",
        "edge_magnitude = sqrt(grad_x.^2 + grad_y.^2);\n",
        "\n",
        "% Display the original and edge-detected images\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imshow(img);\n",
        "title('Original Image');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "imshow(edge_magnitude, []); % Use [] to scale the display range\n",
        "title('Prewitt Edge Detection');"
      ],
      "metadata": {
        "id": "pv3OOV12oZW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: matlab program to perform edge detection using robert filter\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Convert to double for calculations\n",
        "img_double = im2double(img_gray);\n",
        "\n",
        "% Define the Roberts cross kernels\n",
        "roberts_x = [1 0; 0 -1];\n",
        "roberts_y = [0 1; -1 0];\n",
        "\n",
        "% Apply the Roberts filters using convolution\n",
        "% Note: 'valid' is used because Roberts kernels are 2x2,\n",
        "% which is smaller than the standard 'same' padding might imply for larger kernels.\n",
        "% For a 2x2 kernel, 'valid' convolution is often appropriate.\n",
        "grad_x = conv2(img_double, roberts_x, 'valid');\n",
        "grad_y = conv2(img_double, roberts_y, 'valid');\n",
        "\n",
        "% Calculate the gradient magnitude\n",
        "edge_magnitude = sqrt(grad_x.^2 + grad_y.^2);\n",
        "\n",
        "% Display the original and edge-detected images\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imshow(img);\n",
        "title('Original Image');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "imshow(edge_magnitude, []); % Use [] to scale the display range\n",
        "title('Roberts Edge Detection');"
      ],
      "metadata": {
        "id": "rISp8Vn5oZZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: matlab image for apply average filter\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Convert to double for calculations\n",
        "img_double = im2double(img_gray);\n",
        "\n",
        "% Define the average filter kernel (e.g., 3x3)\n",
        "filter_size = 3;\n",
        "average_kernel = ones(filter_size) / (filter_size^2);\n",
        "\n",
        "% Apply the average filter using convolution\n",
        "img_filtered = conv2(img_double, average_kernel, 'same');\n",
        "\n",
        "% Display the original and filtered images\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imshow(img);\n",
        "title('Original Image');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "imshow(img_filtered, []); % Use [] to scale the display range for double image\n",
        "title('Average Filtered Image');"
      ],
      "metadata": {
        "id": "L9BIZZCeoZeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: weighted average filter in matlb\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Convert to double for calculations\n",
        "img_double = im2double(img_gray);\n",
        "\n",
        "% Define the weighted average filter kernel (e.g., a 3x3 Gaussian-like kernel)\n",
        "% The sum of the kernel elements should ideally be 1.\n",
        "weighted_average_kernel = [1 2 1; 2 4 2; 1 2 1];\n",
        "weighted_average_kernel = weighted_average_kernel / sum(weighted_average_kernel(:)); % Normalize the kernel\n",
        "\n",
        "% Define a different example weighted kernel (e.g., emphasizing center pixel more)\n",
        "% weighted_average_kernel = [1 1 1; 1 2 1; 1 1 1];\n",
        "% weighted_average_kernel = weighted_average_kernel / sum(weighted_average_kernel(:)); % Normalize the kernel\n",
        "\n",
        "\n",
        "% Apply the weighted average filter using convolution\n",
        "img_filtered = conv2(img_double, weighted_average_kernel, 'same');\n",
        "\n",
        "% Display the original and filtered images\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imshow(img);\n",
        "title('Original Image');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "imshow(img_filtered, []); % Use [] to scale the display range for double image\n",
        "title('Weighted Average Filtered Image');"
      ],
      "metadata": {
        "id": "YBi9qP6uqmqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a program in Matlab to perform with bit plain slicing on an image\n",
        "\n",
        "% Write a program in Matlab to perform bit plane slicing on an image\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Convert the grayscale image to double for bitwise operations\n",
        "img_double = double(img_gray);\n",
        "\n",
        "% Get the height and width of the image\n",
        "[rows, cols] = size(img_double);\n",
        "\n",
        "% Create a figure to display the bit planes\n",
        "figure;\n",
        "\n",
        "% Loop through each bit plane (0 to 7 for 8-bit image)\n",
        "for bit = 0:7\n",
        "    % Create an empty image for the current bit plane\n",
        "    bit_plane = zeros(rows, cols);\n",
        "\n",
        "    % Extract the bit plane\n",
        "    for i = 1:rows\n",
        "        for j = 1:cols\n",
        "            % Check if the 'bit'-th bit is set (1) for the pixel at (i, j)\n",
        "            % Use bitand and bitshift to check the bit\n",
        "            if bitand(bitshift(img_double(i, j), -bit), 1) == 1\n",
        "                bit_plane(i, j) = 1;\n",
        "            else\n",
        "                bit_plane(i, j) = 0;\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "\n",
        "    % Display the current bit plane\n",
        "    subplot(2, 4, bit + 1); % Arrange plots in a 2x4 grid\n",
        "    imshow(bit_plane, []); % Use [] for automatic scaling, appropriate for binary images\n",
        "    title(['Bit Plane ', num2str(bit)]);\n",
        "end\n",
        "\n",
        "% Optional: Display the original grayscale image\n",
        "figure;\n",
        "imshow(img_gray);\n",
        "title('Original Grayscale Image');"
      ],
      "metadata": {
        "id": "GQqfNBh6qmtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a program in Matlab to perform addition, subtraction and or operation on images\n",
        "\n",
        "% Write a program in Matlab to perform addition, subtraction and or operation on images\n",
        "\n",
        "% Load two images\n",
        "img1 = imread('image1.png'); % Replace with the path to your first image\n",
        "img2 = imread('image2.png'); % Replace with the path to your second image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img1) || isempty(img2)\n",
        "    error('Error: Could not load one or both images.');\n",
        "end\n",
        "\n",
        "% Ensure images are of the same data type and size for operations\n",
        "% Convert to double for arithmetic operations to avoid clipping\n",
        "img1_double = im2double(img1);\n",
        "img2_double = im2double(img2);\n",
        "\n",
        "% Check if images have the same size\n",
        "if ~isequal(size(img1_double), size(img2_double))\n",
        "    error('Error: Images must have the same dimensions for operations.');\n",
        "end\n",
        "\n",
        "% --- Image Addition ---\n",
        "% Direct addition might result in values > 1 for double or > 255 for uint8.\n",
        "% Use imadd or simple addition followed by clipping or scaling if needed.\n",
        "img_addition = imadd(img1_double, img2_double); % imadd handles potential clipping\n",
        "\n",
        "% Display the result of addition\n",
        "figure;\n",
        "subplot(1, 3, 1);\n",
        "imshow(img_addition);\n",
        "title('Image Addition');\n",
        "\n",
        "% --- Image Subtraction ---\n",
        "% Direct subtraction might result in values < 0.\n",
        "% Use imsubtract or simple subtraction followed by clipping.\n",
        "img_subtraction = imsubtract(img1_double, img2_double); % imsubtract handles potential clipping\n",
        "\n",
        "% Display the result of subtraction\n",
        "subplot(1, 3, 2);\n",
        "imshow(img_subtraction);\n",
        "title('Image Subtraction');\n",
        "\n",
        "% --- Image OR Operation (Logical OR) ---\n",
        "% Logical operations are typically performed on binary images or logical arrays.\n",
        "% If your images are not binary, you might need to convert them to logical\n",
        "% or perform a bitwise OR operation (for integer types).\n",
        "% For this example, let's assume we want a logical OR after thresholding\n",
        "% or if the input images are already binary/logical.\n",
        "\n",
        "% Convert double images to logical arrays (thresholding at 0.5)\n",
        "img1_logical = img1_double > 0.5;\n",
        "img2_logical = img2_double > 0.5;\n",
        "\n",
        "% Perform logical OR\n",
        "img_or_logical = img1_logical | img2_logical;\n",
        "\n",
        "% Display the result of logical OR\n",
        "subplot(1, 3, 3);\n",
        "imshow(img_or_logical);\n",
        "title('Image Logical OR');\n",
        "\n",
        "% Adjust figure layout\n",
        "sgtitle('Image Operations'); % Set a super title for the figure\n",
        "\n",
        "% --- Optional: Bitwise OR for integer images ---\n",
        "% If your images are uint8, you can perform bitwise OR directly.\n",
        "% Convert images to uint8 (if they were originally uint8)\n",
        "img1_uint8 = imread('image1.png'); % Reload or cast original\n",
        "img2_uint8 = imread('image2.png'); % Reload or cast original\n",
        "\n",
        "% Ensure they are uint8 and same size\n",
        "if ~isa(img1_uint8, 'uint8') || ~isa(img2_uint8, 'uint8') || ~isequal(size(img1_uint8), size(img2_uint8))\n",
        "     disp('Skipping bitwise OR as images are not uint8 or not same size.');\n",
        "else\n",
        "    img_or_bitwise = bitwiseor(img1_uint8, img2_uint8); % Bitwise OR\n",
        "\n",
        "    figure;\n",
        "    imshow(img_or_bitwise);\n",
        "    title('Image Bitwise OR (for uint8 images)');\n",
        "end"
      ],
      "metadata": {
        "id": "LyUG6KGwqmwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a program in Matlab to perform H detection using Laplacian filter\n",
        "\n",
        "% Write a program in Matlab to perform H detection using Laplacian filter\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Convert to double for calculations\n",
        "img_double = im2double(img_gray);\n",
        "\n",
        "% Define a Laplacian kernel\n",
        "% Common 3x3 Laplacian kernels:\n",
        "% Method 1 (4-connected):\n",
        "laplacian_kernel = [0 1 0; 1 -4 1; 0 1 0];\n",
        "% Method 2 (8-connected):\n",
        "% laplacian_kernel = [1 1 1; 1 -8 1; 1 1 1];\n",
        "\n",
        "% Apply the Laplacian filter using convolution\n",
        "img_laplacian = conv2(img_double, laplacian_kernel, 'same');\n",
        "\n",
        "% The output of the Laplacian filter highlights areas of rapid intensity change (edges and corners).\n",
        "% To \"detect\" features using Laplacian, we often look for zero crossings in the filtered image\n",
        "% or simply display the magnitude of the output.\n",
        "% A common approach for simple detection is to threshold the absolute value of the output.\n",
        "\n",
        "% Display the original and Laplacian filtered images\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imshow(img);\n",
        "title('Original Image');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "% Display the absolute value of the Laplacian output for better visualization\n",
        "imshow(abs(img_laplacian), []); % Use [] to scale the display range\n",
        "title('Laplacian Filter Output (Absolute Value)');\n",
        "\n",
        "% Optional: Threshold the output to get a binary \"detection\" map\n",
        "% Adjust the threshold value as needed based on your image\n",
        "threshold_value = 0.05; % Example threshold\n",
        "edge_map = abs(img_laplacian) > threshold_value;\n",
        "\n",
        "figure;\n",
        "imshow(edge_map);\n",
        "title(['Laplacian Detection (Threshold = ', num2str(threshold_value), ')']);\n"
      ],
      "metadata": {
        "id": "s9QM2usxrTPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write program in Matlab to perform histogram equalisation\n",
        "\n",
        "% Write a program in Matlab to perform histogram equalisation\n",
        "\n",
        "% Load the image\n",
        "img = imread('image.png'); % Replace 'image.png' with the path to your image\n",
        "\n",
        "% Check if image loading was successful\n",
        "if isempty(img)\n",
        "    error('Error: Could not load image.');\n",
        "end\n",
        "\n",
        "% Convert the image to grayscale if it's not already\n",
        "if size(img, 3) == 3\n",
        "    img_gray = rgb2gray(img);\n",
        "else\n",
        "    img_gray = img;\n",
        "end\n",
        "\n",
        "% Perform histogram equalization\n",
        "img_equalized = histeq(img_gray);\n",
        "\n",
        "% Display the original and equalized images\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imshow(img_gray);\n",
        "title('Original Grayscale Image');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "imshow(img_equalized);\n",
        "title('Histogram Equalized Image');\n",
        "\n",
        "% Display the histograms (optional)\n",
        "figure;\n",
        "subplot(1, 2, 1);\n",
        "imhist(img_gray);\n",
        "title('Original Histogram');\n",
        "\n",
        "subplot(1, 2, 2);\n",
        "imhist(img_equalized);\n",
        "title('Equalized Histogram');"
      ],
      "metadata": {
        "id": "1FtN3ftzrTSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a passion program for negative image using discrete Web la transform\n",
        "\n",
        "!pip install PyWavelets\n",
        "\n",
        "import numpy as np\n",
        "import pywt\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to apply DWT and get coefficients\n",
        "def apply_dwt(image):\n",
        "    # Convert image to grayscale if it's color\n",
        "    if len(image.shape) > 2:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Convert to float for DWT\n",
        "    img_float = np.float32(image) / 255.0\n",
        "\n",
        "    # Apply 2D Discrete Wavelet Transform (DWT)\n",
        "    # 'haar' is a simple and commonly used wavelet\n",
        "    coeffs = pywt.wavedec2(img_float, 'haar', level=1)\n",
        "    # coeffs contains: [cA, (cH, cV, cD)]\n",
        "    # cA: Approximation coefficients (low-low frequencies)\n",
        "    # cH: Horizontal detail coefficients (low-high frequencies)\n",
        "    # cV: Vertical detail coefficients (high-low frequencies)\n",
        "    # cD: Diagonal detail coefficients (high-high frequencies)\n",
        "\n",
        "    return coeffs\n",
        "\n",
        "# Function to create a negative image from DWT coefficients\n",
        "def negative_dwt_coefficients(coeffs):\n",
        "    # Make a deep copy to avoid modifying original coefficients\n",
        "    neg_coeffs = [c.copy() if isinstance(c, np.ndarray) else (tuple(map(lambda x: x.copy(), c))) for c in coeffs]\n",
        "\n",
        "    # Get the Approximation coefficients (cA)\n",
        "    cA = neg_coeffs[0]\n",
        "    # Invert the Approximation coefficients (creating a negative)\n",
        "    neg_coeffs[0] = 1.0 - cA\n",
        "\n",
        "    # Note: Inverting only cA creates a negative image.\n",
        "    # You could also choose to invert other coefficients (cH, cV, cD)\n",
        "    # depending on the desired effect, but inverting cA directly relates\n",
        "    # to inverting the low-frequency information, which corresponds\n",
        "    # to the overall intensity of the image.\n",
        "\n",
        "    return neg_coeffs\n",
        "\n",
        "# Function to reconstruct image from negative DWT coefficients\n",
        "def reconstruct_from_dwt(coeffs):\n",
        "    # Reconstruct the image from the modified coefficients\n",
        "    img_reconstructed_float = pywt.waverec2(coeffs, 'haar')\n",
        "\n",
        "    # Ensure pixel values are within [0, 1] range\n",
        "    img_reconstructed_float = np.clip(img_reconstructed_float, 0, 1)\n",
        "\n",
        "    # Convert back to uint8 (0-255)\n",
        "    img_reconstructed_uint8 = np.uint8(img_reconstructed_float * 255)\n",
        "\n",
        "    return img_reconstructed_uint8\n",
        "\n",
        "\n",
        "# Main program\n",
        "# Load the image (replace 'image.png' with your image path)\n",
        "# Upload an image file named 'image.png' to your Colab session or\n",
        "# use a path from your mounted Google Drive, e.g., '/content/drive/My Drive/image.png'\n",
        "img = cv2.imread('image.png')\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img is None:\n",
        "    print(\"Error: Could not load image.\")\n",
        "else:\n",
        "    # Display the original image\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) # Convert BGR to RGB for display\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Apply DWT to the image\n",
        "    original_coeffs = apply_dwt(img)\n",
        "\n",
        "    # Create negative coefficients from the DWT coefficients\n",
        "    negative_coeffs = negative_dwt_coefficients(original_coeffs)\n",
        "\n",
        "    # Reconstruct the image from the negative coefficients\n",
        "    negative_img = reconstruct_from_dwt(negative_coeffs)\n",
        "\n",
        "    # Display the negative image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(negative_img, cmap='gray') # Display as grayscale\n",
        "    plt.title(\"Negative Image (DWT Approach)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # You can optionally save the negative image\n",
        "    # cv2.imwrite('negative_image_dwt.png', negative_img)\n"
      ],
      "metadata": {
        "id": "OZRp5Zg2rTU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a python program to implement login power law transformation using discrete wavelet transform\n",
        "\n",
        "# Function to apply Login Power Law Transformation to DWT Approximation Coefficients\n",
        "def login_power_law_transform_dwt(coeffs, c=1.0, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Applies Login and Power Law transformation to the Approximation coefficients (cA)\n",
        "    of the DWT.\n",
        "\n",
        "    Args:\n",
        "        coeffs (list): List of DWT coefficients obtained from pywt.wavedec2.\n",
        "                       Should be [cA, (cH, cV, cD)].\n",
        "        c (float): Constant for the Login transformation.\n",
        "        gamma (float): Exponent for the Power Law transformation.\n",
        "\n",
        "    Returns:\n",
        "        list: Modified DWT coefficients with transformation applied to cA.\n",
        "    \"\"\"\n",
        "    # Make a deep copy to avoid modifying original coefficients\n",
        "    transformed_coeffs = [c.copy() if isinstance(c, np.ndarray) else (tuple(map(lambda x: x.copy(), c))) for c in coeffs]\n",
        "\n",
        "    # Get the Approximation coefficients (cA)\n",
        "    cA = transformed_coeffs[0]\n",
        "\n",
        "    # Ensure cA values are non-negative before applying log\n",
        "    # Small epsilon to avoid log(0) issues if needed, though cA from Haar are usually okay\n",
        "    cA_clipped = np.maximum(cA, 1e-9) # Clip values close to zero\n",
        "\n",
        "    # Apply Login Transformation: S = c * log(1 + r)\n",
        "    # where r is the input pixel value (cA coefficient)\n",
        "    cA_login = c * np.log(1 + cA_clipped)\n",
        "\n",
        "    # Apply Power Law Transformation: S = c * r^gamma\n",
        "    # where r is the input pixel value (output of login transformation)\n",
        "    cA_login_power_law = np.power(cA_login, gamma)\n",
        "\n",
        "    # Update the Approximation coefficients in the transformed list\n",
        "    transformed_coeffs[0] = cA_login_power_law\n",
        "\n",
        "    return transformed_coeffs\n",
        "\n",
        "# Main program for Login Power Law with DWT\n",
        "# Load the image (replace 'image.png' with your image path)\n",
        "# Upload an image file named 'image.png' to your Colab session or\n",
        "# use a path from your mounted Google Drive, e.g., '/content/drive/My Drive/image.png'\n",
        "img = cv2.imread('image.png')\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img is None:\n",
        "    print(\"Error: Could not load image.\")\n",
        "else:\n",
        "    # Display the original image\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) # Convert BGR to RGB for display\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Apply DWT to the image\n",
        "    original_coeffs = apply_dwt(img) # Reuse the apply_dwt function from before\n",
        "\n",
        "    # Define parameters for Login and Power Law transformations\n",
        "    # Adjust these values to see different effects\n",
        "    c_value = 1.0\n",
        "    gamma_value = 0.5 # Values less than 1 enhance darker regions, values greater than 1 enhance lighter regions\n",
        "\n",
        "    # Apply Login Power Law transformation to DWT Approximation Coefficients\n",
        "    transformed_coeffs = login_power_law_transform_dwt(original_coeffs, c=c_value, gamma=gamma_value)\n",
        "\n",
        "    # Reconstruct the image from the transformed coefficients\n",
        "    transformed_img = reconstruct_from_dwt(transformed_coeffs) # Reuse the reconstruct_from_dwt function\n",
        "\n",
        "    # Display the transformed image\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(transformed_img, cmap='gray') # Display as grayscale\n",
        "    plt.title(f\"Login Power Law (c={c_value}, γ={gamma_value})\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Optional: Display the image after only Login transformation\n",
        "    # This requires a slight modification or a separate call\n",
        "    login_only_coeffs = login_power_law_transform_dwt(original_coeffs, c=c_value, gamma=1.0)\n",
        "    login_only_img = reconstruct_from_dwt(login_only_coeffs)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(login_only_img, cmap='gray')\n",
        "    plt.title(f\"Login Transform (c={c_value})\")\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # You can optionally save the transformed image\n",
        "    # cv2.imwrite('login_power_law_dwt_image.png', transformed_img)\n"
      ],
      "metadata": {
        "id": "c577xExUqmzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a python program for image fusion with average operation using discrete web Lee transform\n",
        "\n",
        "# Function to fuse DWT coefficients using average operation\n",
        "def fuse_dwt_average(coeffs1, coeffs2):\n",
        "    \"\"\"\n",
        "    Fuses two sets of DWT coefficients using the average rule for all subbands.\n",
        "\n",
        "    Args:\n",
        "        coeffs1 (list): DWT coefficients from the first image [cA1, (cH1, cV1, cD1)].\n",
        "        coeffs2 (list): DWT coefficients from the second image [cA2, (cH2, cV2, cD2)].\n",
        "\n",
        "    Returns:\n",
        "        list: Fused DWT coefficients [cAfused, (cHfused, cVfused, cDfused)].\n",
        "    \"\"\"\n",
        "    # Check if the coefficient structures match (same number of levels and subbands)\n",
        "    if len(coeffs1) != len(coeffs2):\n",
        "        raise ValueError(\"Coefficient structures do not match for fusion.\")\n",
        "\n",
        "    fused_coeffs = []\n",
        "\n",
        "    # Fuse Approximation coefficients (cA)\n",
        "    cA1, detail_coeffs1 = coeffs1[0], coeffs1[1:]\n",
        "    cA2, detail_coeffs2 = coeffs2[0], coeffs2[1:]\n",
        "\n",
        "    # Average the Approximation coefficients\n",
        "    cA_fused = (cA1 + cA2) / 2.0\n",
        "    fused_coeffs.append(cA_fused)\n",
        "\n",
        "    # Fuse Detail coefficients (cH, cV, cD) for each level\n",
        "    for i in range(len(detail_coeffs1)):\n",
        "        cH1, cV1, cD1 = detail_coeffs1[i]\n",
        "        cH2, cV2, cD2 = detail_coeffs2[i]\n",
        "\n",
        "        # Average the Detail coefficients\n",
        "        cH_fused = (cH1 + cH2) / 2.0\n",
        "        cV_fused = (cV1 + cV2) / 2.0\n",
        "        cD_fused = (cD1 + cD2) / 2.0\n",
        "\n",
        "        fused_coeffs.append((cH_fused, cV_fused, cD_fused))\n",
        "\n",
        "    return fused_coeffs\n",
        "\n",
        "# Main program for Image Fusion with Average Operation using DWT\n",
        "# Load the two images (replace 'image1.png' and 'image2.png' with your image paths)\n",
        "# Upload image files named 'image1.png' and 'image2.png' to your Colab session or\n",
        "# use paths from your mounted Google Drive.\n",
        "img1 = cv2.imread('image1.png')\n",
        "img2 = cv2.imread('image2.png')\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img1 is None or img2 is None:\n",
        "    print(\"Error: Could not load one or both images.\")\n",
        "else:\n",
        "    # Ensure images have the same dimensions for fusion\n",
        "    if img1.shape != img2.shape:\n",
        "        print(\"Error: Images must have the same dimensions for fusion.\")\n",
        "    else:\n",
        "        # Display the original images\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Image 1\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Image 2\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Apply DWT to both images\n",
        "        coeffs1 = apply_dwt(img1) # Reuse the apply_dwt function\n",
        "        coeffs2 = apply_dwt(img2) # Reuse the apply_dwt function\n",
        "\n",
        "        # Fuse the DWT coefficients using the average rule\n",
        "        fused_coeffs = fuse_dwt_average(coeffs1, coeffs2)\n",
        "\n",
        "        # Reconstruct the image from the fused coefficients\n",
        "        fused_img_dwt_avg = reconstruct_from_dwt(fused_coeffs) # Reuse the reconstruct_from_dwt function\n",
        "\n",
        "        # Display the fused image\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(fused_img_dwt_avg, cmap='gray') # Display as grayscale\n",
        "        plt.title(\"Fused Image (DWT + Average)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # You can optionally save the fused image\n",
        "        # cv2.imwrite('fused_image_dwt_average.png', fused_img_dwt_avg)\n"
      ],
      "metadata": {
        "id": "mbSwI7oXr44U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Program for image argumentation on an input image without using def\n",
        "\n",
        "# Install Albumentations if not already installed\n",
        "!pip install -U albumentations\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "# Load the image\n",
        "# Replace 'image.png' with the path to your image\n",
        "# Upload an image file named 'image.png' to your Colab session or\n",
        "# use a path from your mounted Google Drive, e.g., '/content/drive/My Drive/image.png'\n",
        "img_path = 'image.png' # Ensure you have an image file named 'image.png'\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img is None:\n",
        "    print(f\"Error: Could not load image from {img_path}.\")\n",
        "else:\n",
        "    # Albumentations expects RGB format for display\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Define the augmentation pipeline\n",
        "    # This is a combination of different augmentations applied sequentially\n",
        "    transform = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),            # Flip horizontally with 50% probability\n",
        "        A.RandomBrightnessContrast(p=0.2),  # Randomly change brightness and contrast\n",
        "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.4), # Random affine transformation\n",
        "        A.GaussNoise(p=0.1),                # Add Gaussian noise\n",
        "        A.ColorJitter(p=0.3, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Randomly change color properties\n",
        "        A.Resize(height=256, width=256)     # Resize the image\n",
        "    ])\n",
        "\n",
        "    # Apply the transformation to the image\n",
        "    augmented_image = transform(image=img_rgb)['image']\n",
        "\n",
        "    # Display the original and augmented images\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(augmented_image)\n",
        "    plt.title(\"Augmented Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # You can apply the augmentation multiple times to see different results\n",
        "    # augmented_image_2 = transform(image=img_rgb)['image']\n",
        "    # plt.figure(); plt.imshow(augmented_image_2); plt.title(\"Another Augmented Image\"); plt.axis('off'); plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ssCHFeIyr47E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a python program for load and convert the video into frames using open CV\n",
        "\n",
        "# Replace 'your_video.mp4' with the path to your video file\n",
        "video_path = 'your_video.mp4'\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if the video file was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_path}\")\n",
        "else:\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        # Read a frame from the video\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was read successfully (ret is True)\n",
        "        if ret:\n",
        "            # Process the frame (e.g., convert to grayscale, resize, save)\n",
        "            # Example: Save the frame as an image file\n",
        "            frame_filename = f'frame_{frame_count:04d}.png' # Format filename with leading zeros\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            print(f'Saved {frame_filename}')\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "            # Optional: Display the frame (can be slow for long videos)\n",
        "            # cv2.imshow('Video Frame', frame)\n",
        "\n",
        "            # Optional: Break the loop if a key is pressed (e.g., 'q')\n",
        "            # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "            #     break\n",
        "        else:\n",
        "            # Break the loop if there are no more frames\n",
        "            break\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Close all OpenCV windows (if any were opened)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    print(f\"Finished processing video. Extracted {frame_count} frames.\")\n",
        "\n",
        "# After running the code, the extracted frames will be saved in the Colab workspace.\n",
        "# You can view them using the file browser on the left or download them."
      ],
      "metadata": {
        "id": "rGOtLpejr49r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a python program for image fusion using discrete a stationary Beyblade transform\n",
        "\n",
        "# Write a python program for image fusion using discrete a stationary Beyblade transform\n",
        "\n",
        "# Note: \"discrete a stationary Beyblade transform\" appears to be a typo.\n",
        "# Assuming you meant \"Discrete Stationary Wavelet Transform\" (SWT).\n",
        "# SWT is translation invariant, which can be beneficial for image fusion compared to DWT.\n",
        "\n",
        "# Let's implement image fusion using SWT (Stationary Wavelet Transform)\n",
        "\n",
        "!pip install PyWavelets\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "# Function to apply SWT and get coefficients\n",
        "def apply_swt(image, wavelet='haar', level=1):\n",
        "    \"\"\"\n",
        "    Applies the Stationary Wavelet Transform (SWT) to an image.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input image (preferably grayscale).\n",
        "        wavelet (str): The name of the wavelet to use (e.g., 'haar', 'db1', 'sym2').\n",
        "        level (int): The decomposition level.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of coefficient tuples for each level.\n",
        "              coeffs[i] = (cA_i, (cH_i, cV_i, cD_i)) for level i+1 (0-indexed).\n",
        "              The last element is (cH_L, cV_L, cD_L) for the highest level L.\n",
        "    \"\"\"\n",
        "    # Convert image to grayscale if it's color\n",
        "    if len(image.shape) > 2:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Convert to float for SWT\n",
        "    img_float = np.float32(image) / 255.0\n",
        "\n",
        "    # Apply 2D Stationary Wavelet Transform (SWT)\n",
        "    # Returns a list of tuples, one tuple per level.\n",
        "    # Each tuple contains (cA, (cH, cV, cD)) for that level's decomposition.\n",
        "    coeffs = pywt.swtn(img_float, wavelet, level=level)\n",
        "\n",
        "    return coeffs\n",
        "\n",
        "# Function to fuse SWT coefficients\n",
        "def fuse_swt_coefficients(coeffs1, coeffs2, fusion_rule_approx='average', fusion_rule_detail='max'):\n",
        "    \"\"\"\n",
        "    Fuses two sets of SWT coefficients using specified fusion rules.\n",
        "\n",
        "    Args:\n",
        "        coeffs1 (list): SWT coefficients from the first image [cA1_L-1, (cH1_L-1, cV1_L-1, cD1_L-1), ..., (cH1_0, cV1_0, cD1_0)].\n",
        "        coeffs2 (list): SWT coefficients from the second image [cA2_L-1, (cH2_L-1, cV2_L-1, cD2_L-1), ..., (cH2_0, cV2_0, cD2_0)].\n",
        "        fusion_rule_approx (str): Fusion rule for the Approximation coefficients ('average', 'min', 'max').\n",
        "        fusion_rule_detail (str): Fusion rule for the Detail coefficients ('average', 'max', 'min', 'mean_abs').\n",
        "\n",
        "    Returns:\n",
        "        list: Fused SWT coefficients in the same structure as input.\n",
        "    \"\"\"\n",
        "    if len(coeffs1) != len(coeffs2):\n",
        "        raise ValueError(\"Coefficient structures do not match for fusion.\")\n",
        "\n",
        "    fused_coeffs = []\n",
        "\n",
        "    # The last element in swtn coeffs is the approximation coefficients for the highest level\n",
        "    # and the detail coefficients for that level.\n",
        "    # The preceding elements are detail coefficients for lower levels.\n",
        "    # Example for level=2: [(cH1_1, cV1_1, cD1_1), (cH1_0, cV1_0, cD1_0)]\n",
        "    # The output of swtn is structured as [(cH_L-1, cV_L-1, cD_L-1), ..., (cH_0, cV_0, cD_0)].\n",
        "    # Wait, the documentation for pywt.swtn suggests it returns\n",
        "    # [(cA_L, (cH_L, cV_L, cD_L)), (cH_L-1, cV_L-1, cD_L-1), ..., (cH_0, cV_0, cD_0)]\n",
        "    # Let's verify this or use a simpler structure if it's just detail bands.\n",
        "    # Re-reading the pywt.swtn docs: \"Returns a list of coefficient tuples.\n",
        "    # For level `k`, element `k` is a tuple of (cH_k, cV_k, cD_k).\n",
        "    # For a decomposition at level `N`, list length is `N`.\n",
        "    # To get the approximation coefficients cA_N, you would use the last element of the output\n",
        "    # of pywt.iswtn applied to only the approximation part of the coefficients.\n",
        "    # This is a bit different from DWT's wavedec2/waverec2.\n",
        "\n",
        "    # Let's adapt to the swtn structure which seems to return detail coefficients per level.\n",
        "    # We need a way to handle the approximation part for fusion.\n",
        "    # A common approach for SWT-based fusion is to fuse approximation and detail coefficients separately.\n",
        "    # pywt.iswtn takes the list of detail coefficient tuples. It seems it doesn't directly give cA.\n",
        "\n",
        "    # Let's adjust the approach: Instead of fusing cA explicitly from swtn output,\n",
        "    # we'll fuse the detail coefficients and then handle the reconstruction which inherently\n",
        "    # combines detail coefficients across levels. The approximation part is implicitly handled\n",
        "    # by the inverse SWT reconstruction process when using only detail coefficients.\n",
        "    # Or, we can treat the L-level decomposition as having an implicit cA part at level L.\n",
        "\n",
        "    # Let's assume pywt.swtn(img, wavelet, level=L) gives\n",
        "    # [(cH_L, cV_L, cD_L), (cH_L-1, cV_L-1, cD_L-1), ..., (cH_1, cV_1, cD_1)]\n",
        "    # The base approximation would be obtained by reconstructing from an empty list of details.\n",
        "    # This seems complex. Let's use the standard DWT approach with SWT properties in mind.\n",
        "\n",
        "    # Let's reconsider the swtn structure and standard fusion literature.\n",
        "    # In standard SWT fusion, the approximation subband at the highest level is often\n",
        "    # fused using averaging, while detail subbands are fused using a rule like 'max absolute value'.\n",
        "    # pywt's swtn returns [(cH_L, cV_L, cD_L), ..., (cH_1, cV_1, cD_1)].\n",
        "    # There is no explicit cA in the output of swtn directly.\n",
        "    # The reconstruction is done using iswtn(coeffs, wavelet).\n",
        "\n",
        "    # A common workaround when cA is not directly available from swtn is to\n",
        "    # perform DWT to get cA, fuse cA, perform SWT to get details, fuse details,\n",
        "    # and then try to combine them. This is not ideal.\n",
        "\n",
        "    # Let's try a different interpretation or approach commonly used:\n",
        "    # Fuse the highest level detail coefficients using a rule,\n",
        "    # and the lower level detail coefficients using another rule.\n",
        "    # The 'approximation' aspect in SWT comes from the reconstruction process.\n",
        "\n",
        "    # Let's go with a simplified fusion: fuse all detail subbands across all levels.\n",
        "    # The approximation part will be implicitly handled by the inverse transform.\n",
        "    # This might not be the most standard SWT fusion, but it's directly applicable\n",
        "    # to the output of pywt.swtn.\n",
        "\n",
        "    # Example of swtn output for level=2: [(cH2, cV2, cD2), (cH1, cV1, cD1)]\n",
        "    # Each element is a tuple of (cH, cV, cD) for a given level (from highest to lowest detail).\n",
        "\n",
        "    # Iterate through levels (from highest detail level down to lowest detail level)\n",
        "    for i in range(len(coeffs1)):\n",
        "        (cH1, cV1, cD1) = coeffs1[i]\n",
        "        (cH2, cV2, cD2) = coeffs2[i]\n",
        "\n",
        "        # Fuse Horizontal Detail coefficients\n",
        "        if fusion_rule_detail == 'average':\n",
        "            cH_fused = (cH1 + cH2) / 2.0\n",
        "        elif fusion_rule_detail == 'max':\n",
        "            cH_fused = np.maximum(cH1, cH2)\n",
        "        elif fusion_rule_detail == 'min':\n",
        "            cH_fused = np.minimum(cH1, cH2)\n",
        "        elif fusion_rule_detail == 'mean_abs':\n",
        "            # Fuse based on maximum absolute value (common for details)\n",
        "            cH_fused = np.where(np.abs(cH1) > np.abs(cH2), cH1, cH2)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown detail fusion rule: {fusion_rule_detail}\")\n",
        "\n",
        "        # Fuse Vertical Detail coefficients\n",
        "        if fusion_rule_detail == 'average':\n",
        "            cV_fused = (cV1 + cV2) / 2.0\n",
        "        elif fusion_rule_detail == 'max':\n",
        "            cV_fused = np.maximum(cV1, cV2)\n",
        "        elif fusion_rule_detail == 'min':\n",
        "            cV_fused = np.minimum(cV1, cV2)\n",
        "        elif fusion_rule_detail == 'mean_abs':\n",
        "            cV_fused = np.where(np.abs(cV1) > np.abs(cV2), cV1, cV2)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown detail fusion rule: {fusion_rule_detail}\")\n",
        "\n",
        "        # Fuse Diagonal Detail coefficients\n",
        "        if fusion_rule_detail == 'average':\n",
        "            cD_fused = (cD1 + cD2) / 2.0\n",
        "        elif fusion_rule_detail == 'max':\n",
        "            cD_fused = np.maximum(cD1, cD2)\n",
        "        elif fusion_rule_detail == 'min':\n",
        "            cD_fused = np.minimum(cD1, cD2)\n",
        "        elif fusion_rule_detail == 'mean_abs':\n",
        "            cD_fused = np.where(np.abs(cD1) > np.abs(cD2), cD1, cD2)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown detail fusion rule: {fusion_rule_detail}\")\n",
        "\n",
        "        fused_coeffs.append((cH_fused, cV_fused, cD_fused))\n",
        "\n",
        "    # Note: In this simplified approach using pywt.swtn directly,\n",
        "    # we are only fusing the detail coefficients across levels.\n",
        "    # The approximation component is not explicitly fused but emerges from iswtn.\n",
        "    # A more sophisticated SWT fusion might involve deriving the highest level\n",
        "    # approximation coefficient and fusing it separately, then reconstructing.\n",
        "    # However, pywt.swtn/iswtn structure makes this less direct than wavedec2/waverec2.\n",
        "\n",
        "    # Let's stick to fusing only the detail coefficients as provided by swtn.\n",
        "    # If you need to fuse approximation coefficients explicitly, you might\n",
        "    # need to use pywt.wavedec2 and then handle the stationary aspect,\n",
        "    # which is more involved (requires periodic padding and keeping all shifts).\n",
        "\n",
        "    return fused_coeffs # Returns a list of detail coefficient tuples per level\n",
        "\n",
        "# Function to reconstruct image from SWT coefficients\n",
        "def reconstruct_from_swt(coeffs, wavelet='haar'):\n",
        "    \"\"\"\n",
        "    Reconstructs an image from SWT coefficients.\n",
        "\n",
        "    Args:\n",
        "        coeffs (list): SWT coefficients obtained from pywt.swtn.\n",
        "        wavelet (str): The name of the wavelet used for decomposition.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Reconstructed image (uint8).\n",
        "    \"\"\"\n",
        "    # Reconstruct the image from the detail coefficients\n",
        "    # pywt.iswtn takes the list of detail coefficient tuples per level.\n",
        "    img_reconstructed_float = pywt.iswtn(coeffs, wavelet)\n",
        "\n",
        "    # Ensure pixel values are within [0, 1] range\n",
        "    img_reconstructed_float = np.clip(img_reconstructed_float, 0, 1)\n",
        "\n",
        "    # Convert back to uint8 (0-255)\n",
        "    img_reconstructed_uint8 = np.uint8(img_reconstructed_float * 255)\n",
        "\n",
        "    return img_reconstructed_uint8\n",
        "\n",
        "# Main program for Image Fusion using Stationary Wavelet Transform (SWT)\n",
        "# Load the two images (replace 'image1.png' and 'image2.png' with your image paths)\n",
        "# Upload image files named 'image1.png' and 'image2.png' to your Colab session or\n",
        "# use paths from your mounted Google Drive.\n",
        "# Example:\n",
        "# !wget https://www.ece.rice.edu/~dror/bandlet/demos/images/lena.png -O image1.png\n",
        "# !wget https://www.ece.rice.edu/~dror/bandlet/demos/images/barbara.png -O image2.png\n",
        "# Make sure images exist or upload them.\n",
        "\n",
        "img1 = cv2.imread('image1.png') # Path to first image\n",
        "img2 = cv2.imread('image2.png') # Path to second image\n",
        "\n",
        "# Check if image loading was successful\n",
        "if img1 is None or img2 is None:\n",
        "    print(\"Error: Could not load one or both images. Make sure 'image1.png' and 'image2.png' exist.\")\n",
        "else:\n",
        "    # Ensure images have the same dimensions for fusion\n",
        "    if img1.shape != img2.shape:\n",
        "        print(\"Error: Images must have the same dimensions for fusion.\")\n",
        "    else:\n",
        "        # Display the original images\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Image 1\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"Image 2\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Define wavelet and decomposition level\n",
        "        wavelet_name = 'haar' # Choose a wavelet\n",
        "        decomposition_level = 1 # Choose decomposition level (e.g., 1, 2, 3)\n",
        "\n",
        "        # Apply SWT to both images\n",
        "        # swtn returns detail coefficients for levels 1 to `level`.\n",
        "        # The output structure is [(cH_L, cV_L, cD_L), ..., (cH_1, cV_1, cD_1)]\n",
        "        coeffs1 = apply_swt(img1, wavelet=wavelet_name, level=decomposition_level)\n",
        "        coeffs2 = apply_swt(img2, wavelet=wavelet_name, level=decomposition_level)\n",
        "\n",
        "        # Fuse the SWT coefficients\n",
        "        # We'll use 'mean_abs' for detail coefficient fusion, which is common.\n",
        "        # Since swtn doesn't explicitly provide cA, we'll fuse details.\n",
        "        fused_coeffs_swt = fuse_swt_coefficients(coeffs1, coeffs2, fusion_rule_detail='mean_abs')\n",
        "\n",
        "        # Reconstruct the image from the fused coefficients\n",
        "        fused_img_swt = reconstruct_from_swt(fused_coeffs_swt, wavelet=wavelet_name)\n",
        "\n",
        "        # Display the fused image\n",
        "        plt.subplot(1, 3, 3)\n",
        "        # SWT reconstruction using only detail coefficients might result in an image\n",
        "        # where overall brightness is different. Using cmap='gray' is appropriate.\n",
        "        plt.imshow(fused_img_swt, cmap='gray')\n",
        "        plt.title(f\"Fused Image (SWT '{wavelet_name}', Level {decomposition_level})\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # You can optionally save the fused image\n",
        "        # cv2.imwrite('fused_image_swt.png', fused_img_swt)\n",
        "\n"
      ],
      "metadata": {
        "id": "mq_M8dAAr4_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a python program for CNN model with data communication on CIFR 10 data set\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install tensorflow keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices (one-hot encoding)\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    # Convolutional Layer 1\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Convolutional Layer 3\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "\n",
        "    # Flatten layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Dense layers\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5), # Dropout for regularization\n",
        "    Dense(num_classes, activation='softmax') # Output layer with softmax for classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "# Using smaller epochs for demonstration; increase for better accuracy\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# --- Data Communication Aspect (Simulated) ---\n",
        "# In a real distributed system, data communication happens during:\n",
        "# 1. Data loading/sharding across nodes.\n",
        "# 2. Parameter synchronization during training (e.g., in distributed training frameworks).\n",
        "# 3. Sending model or gradient updates between nodes.\n",
        "\n",
        "# For this single-notebook example, we can simulate data communication conceptually\n",
        "# by showing how data might be prepared or how results might be exchanged.\n",
        "\n",
        "# Example: Simulate sending a batch of data for prediction\n",
        "def simulate_data_sending(data_batch):\n",
        "  \"\"\"Simulates sending a batch of image data over a network.\"\"\"\n",
        "  print(f\"\\nSimulating sending a batch of data with shape: {data_batch.shape}\")\n",
        "  # In a real scenario, this would involve serialization (e.g., pickle, protobuf)\n",
        "  # and sending data via sockets, gRPC, or a message queue.\n",
        "  # For simulation, we just acknowledge the data structure.\n",
        "\n",
        "# Example: Simulate receiving model parameters\n",
        "def simulate_parameter_receiving(parameters):\n",
        "  \"\"\"Simulates receiving model parameters over a network.\"\"\"\n",
        "  print(\"\\nSimulating receiving model parameters.\")\n",
        "  # In a real scenario, this would involve deserialization and loading parameters\n",
        "  # into a local model.\n",
        "  # For simulation, we just acknowledge the structure (a list of numpy arrays).\n",
        "  # print(f\"Received parameters for {len(parameters)} layers.\")\n",
        "  # for i, param in enumerate(parameters):\n",
        "  #   print(f\"  Layer {i}: shape {param.shape}, dtype {param.dtype}\")\n",
        "\n",
        "\n",
        "# Simulate data communication during prediction\n",
        "# Take a small batch from the test set\n",
        "sample_batch = x_test[:16]\n",
        "simulate_data_sending(sample_batch)\n",
        "\n",
        "# Simulate receiving the model's weights (parameters)\n",
        "model_weights = model.get_weights()\n",
        "simulate_parameter_receiving(model_weights)\n",
        "\n",
        "\n",
        "# Plot training history (accuracy and loss)\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions (example)\n",
        "predictions = model.predict(x_test[:10])\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y_test[:10], axis=1)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(10):\n",
        "    print(f\"Image {i}: Predicted={predicted_classes[i]}, True={true_classes[i]}\")\n",
        "\n",
        "# Display some test images and their predicted labels\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_test[i])\n",
        "    predicted_label = class_names[predicted_classes[i]]\n",
        "    true_label = class_names[true_classes[i]]\n",
        "    color = 'blue' if predicted_classes[i] == true_classes[i] else 'red'\n",
        "    plt.xlabel(f'{predicted_label} ({true_label})', color=color)\n",
        "plt.suptitle('Sample CIFAR-10 Test Images with Predictions', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IE2EfReCnJMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}